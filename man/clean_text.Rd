% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/01_preprocessing.R
\name{clean_text}
\alias{clean_text}
\title{Clean and Normalize Text}
\usage{
clean_text(
  x,
  text_col = "text",
  id_col = NULL,
  group_col = NULL,
  replace_emojis = TRUE,
  replace_alphaless = TRUE,
  max_char = 2000,
  tokenize_sentences = TRUE,
  return_string = FALSE,
  verbose = TRUE
)
}
\arguments{
\item{x}{Character vector or data.frame containing texts to clean.}

\item{text_col}{Character, name of the text column if \code{x} is a data.frame.
Ignored if \code{x} is a character vector. Default = "text".}

\item{id_col}{Character, optional column name in the input data.frame to
preserve as an identifier column named \code{id}. Default = NULL.}

\item{group_col}{Character, optional column name in the input data.frame
to preserve as a grouping variable. Default = NULL.}

\item{replace_emojis}{Logical, whether to replace emojis with placeholder
names. Default = TRUE.}

\item{replace_alphaless}{Logical, whether to replace strings that contain no
alphabetic characters with empty strings. Default = TRUE.}

\item{max_char}{Integer, maximum number of characters per text. Texts longer
than this are truncated. Default = 2000.}

\item{tokenize_sentences}{Logical, whether to split texts with more than
3 sentences into individual sentences. Default = TRUE.}

\item{return_string}{Logical, if TRUE, return only the cleaned character
vector (\code{text_clean}) instead of a full data.table. Default = FALSE.}

\item{verbose}{Logical, whether to print progress messages. Default = TRUE.}
}
\value{
Either:
\itemize{
\item A data.table with the columns described above.
\item Or a character vector of cleaned texts if \code{return_string = TRUE}.
}
}
\description{
Cleans and optionally tokenizes raw text data. Handles emoji replacement,
removal of unsupported characters, number and punctuation normalization,
trimming, squishing whitespace, and limiting text length. Returns either
a standardized data.table or a character vector.
}
\details{
The cleaning pipeline performs the following steps:
\enumerate{
\item Convert input to UTF-8 and replace NA with empty string.
\item Optionally replace emojis with placeholder names.
\item Remove unsupported characters (keep only letters, numbers, punctuation, whitespace).
\item Normalize encoding and truncate texts exceeding \code{max_char}.
\item Optionally remove texts without alphabetic characters.
\item Replace curly quotes with straight quotes.
\item Normalize numbers so that dots between digits or trailing dots are replaced with spaces
(e.g. \code{3.4.} becomes \code{3 4}).
\item Collapse repeated quotation marks to a single one.
\item Squish whitespace.
\item Optionally tokenize texts with more than 3 sentences into separate rows.
}

Sentence tokenization ensures each row has a unique \code{row_id} suffixed
with the sentence number (e.g. \code{"12_1"}, \code{"12_2"}). Non-tokenized
rows also receive a \code{"_1"} suffix for consistency. The column
\code{sen_id} is always present and set to 1 if not tokenized.

If \code{group_col} is specified, it is preserved in the output.

The returned data.table contains the following columns:
\itemize{
\item \code{row_id}: Unique identifier for each cleaned row.
\item \code{sen_id}: Sentence index (1 for non-tokenized).
\item \code{id}: User-provided identifier, if available.
\item \code{<group_col>}: Optional grouping column, if provided.
\item \code{text_orig}: Original text before cleaning.
\item \code{text_clean}: Cleaned and normalized text.
}
}
